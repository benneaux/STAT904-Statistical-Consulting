---
title: "Assignment 6"
author: "Benjamin Moran \n c3076448 (University of Newcastle)"
date: 'Date Generated: `r Sys.Date()`'
# bibliography: AssX.bib
output:
  pdf_document: default
  html_document:
    df_print: tibble
    fig_caption: yes
subtitle: STAT904 Statistical Consulting
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse)
library(sandwich)
library(MCMCpack)
```

#### Plagiarism Statement 
No part of this Assignment has been copied from anyone else, and I have not lent any part of it to anyone else.

# Question 1

Identify 2 modern sources that consider how to select statistical analysis procedures. Provide a one-page summary what you learnt from them.

## R Bloggers

This is a blog that aggregates dozens of other `R` related blogs and posts from around the web, including content from major corporations (e.g. Microsoft and information about cross-compatibility with their products), academics (posts by __Rob Hyndman__ are particularly relevant to my field of time series analysis),
R foundation employees (__Hadley Wickham__ (tidyverse and ggplot) is probably the most useful to me) and other assorted educators/coders.

Whilst not strictly a source of information for choosing statistical analysis, the wide scope of contenct assures that a lot of this type of information is included alongside other information about how to do a number of things relevant to statistical consulting. One thing of note is series of posts detailing project organisation and workflow paradigms that I have begun to implement in both my professional and academic lives. 

Bulletins about new package releases/versions are also included, which helps me keep abreast of new methods for statistical analyses that 

But the key is that the shear number of educators and practitioners who author posts dealing with practical analyses methods means that if I am unsure how to proceed with a type of analysis I'm not familiar with, the information that I need to proceed will probably be aggregated here.

# Question 2

The number of fatal accidents on a stretch of road over a 10 year period has been:
$$
0~2~1~0~3~1~2~0~1~0
$$
After lowering the speed limit to 80kms an hour the number of fatal accidents in the following five years is:

$$
0~0~0~1~0
$$
Users of the road are lobbying for the speed limit to be increased back to the original 100kms an hour. They say that the drop in the number of fatalities after the lowering of the speed limit is due to chance. 

The consulting company you work for is contracted to assess the effectiveness of the reduction in the speed limit and particularly whether the position of the road users is valid statistically.

## a) 

List 6 questions you would ask about this project.

## b) 

_Undertake whatever analyses and calculations you think are useful. Write a report of no more than three pages. Clearly state your conclusions about the road users assertion that the observed effect of the lowering of the speed limit is due to chance._

### Preliminary observations.

Unfortunately, we do not have much data available to us and the data that we do have has a number of zeroes, but that should not present too much of an issue. In the following analysis I am going to assume that:

* The data is complete - i.e. that no fatal accidents have been left out of the data set.
* That there is nothing to distinguish between types of fatal accidents - i.e. those involving one car, two cars, or one or more cars and a pedestrian.

I doubt that the last assumption would hold up under scrutiny; you should therefore investigate the types of accidents further.

### Data

The data consists of an aggregate count (integer values) of fatal accidents on this stretch of road for each year in a ten year period. Because we are considering an aggregate for each year we will not concern ourselves with whether or not there are excessive zeroes in the data.

### Analysis

Because we are analysing counts we will assume that the data is being generated by a _Poisson Process_. 

```{r}
# https://www.colorado.edu/physics/phys2150/phys2150_sp12/PHYS2150/Lecture_Notes_files/phys2150_lect6_sp12.pdf
# library(tidyverse)
# library(sandwich)
data = matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,
                0,2,1,0,3,1,2,0,1,0,0,0,0,1,0,
                0,0,0,0,0,0,0,0,0,0,1,1,1,1,1),
              # byrow = TRUE,
              ncol = 3) %>% as.data.frame()
colnames(data) = c("Year","Deaths", "SpeedReduced")
data$SpeedReduced = factor(data$SpeedReduced, labels = c("No","Yes"))
knitr::kable(data)
```

An assumption underlying our use of the Poisson is that the mean and variance of the data are approximately the same. We can take subsets of the data representing before and after the intervention and then calculate these statistics.

```{R}
with(data, tapply(Deaths, SpeedReduced, function(x) {
  sprintf("M (Var) = %1.2f (%1.2f)", mean(x), var(x))
}))
```

These are approximately equal so we can continue with our analysis. We will first fit a basic _generalised linear model (glm)_ to the data, specifying that _Deaths_ are our response variable and _SpeedReduced_ is our predictor.

```{r}
summary(m1 <- glm(Deaths ~ SpeedReduced, family=poisson(), data=data))
```

The output indicates that the speed reduction did not significantly effect the number of fatal accidents. But - as stated previously - we are not working from much data so it is prudent to investigate further. Firstly, we can check to see how well the above model fits the data. By comparing the _residual deviance_ to the _deviance_ expected under the null model, we can get an idea of whether or not the hypothesised model is a good fit for the data.

```{r}
with(m1, cbind(res.deviance = deviance, df = df.residual,
               p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```

The reported p value is not significant, so we assume that the model is a good fit for the data.

Next, we can aleviate any issues that may arise from the fact that the conditional means and variances are not _exactly_ equal by estimating and using __robust standard errors__ to produce both confidence intervals and p values for the reported regression parameters (Intercept and SpeedReduced). 

```{r}
cov.m1 <- vcovHC(m1, type="HC0")
std.err <- sqrt(diag(cov.m1))
r.est <- cbind(Estimate= coef(m1), "Robust SE" = std.err,
               "Pr(>|z|)" = 2 * pnorm(abs(coef(m1)/std.err), lower.tail=FALSE),
               LL = coef(m1) - 1.96 * std.err,
               UL = coef(m1) + 1.96 * std.err)

r.est
```

Whilst not significant, this more robust analysis does shift the p value attributed to the speed reduction closer to a value that we would in general consider to be significant $(\alpha = 0.05)$.

We should also check to see if the model with speed reduction is a predictor is any better than the simple model without it. We can compare the two models using a __Chi-squared test__.

```{r}
## update m1 model dropping our predictor
m2 <- update(m1, . ~ . - SpeedReduced)
## test model differences with chi square test
anova(m2, m1, test="Chisq")
```

The results indicate that the m

However, this doesn't necessarily invalidate your claim that a separate process is the cause of the reduction in fatal accidents. To test this idea we will need to 

```{r}
summary(m1 <- glm(Deaths ~ SpeedReduced, family="poisson", data=data))
with(m1, cbind(res.deviance = deviance, df = df.residual,
               p = pchisq(deviance, df.residual, lower.tail=FALSE)))
(s1 <- data.frame(Deaths = mean(data$Deaths),
                  SpeedReduced = factor(1:2, levels = 1:2, labels = c("No","Yes"))))
predict(m1, s1, type="response", se.fit=TRUE)
```

Because of the small data set it may be sensible to use simulation to establish both if a breakpoint exists/if the generating process for the data changes and - if so - where that breakpoint is. By __breakpoint__ I mean the point in the data set where the process that actually resulted in the reduction in fatal accidents begins. If the local council (or whomever is responsible for changing the speed limit) is correct, then we should be able to identify a break at the 11th point, corresponding to the reduction in speed. If you are correct and the reduction is by chance then we should not be able to identify a point. Also, if the reduction is due to some other factor, then we may see a breakpoint that preceeds the reduction in speed limit.

We will conduct this analysis using 

<!-- https://www.colorado.edu/ibs/crs/workshops/R_for_BayesianP10-24-2008_Lu/Bayesian_R.pdf -->
<!-- http://learnbayes.blogspot.com.au/2007/12/poisson-change-point-model.html -->

```{r}
# library(MCMCpack)
Y <- c(0,2,1,0,3,1,2,0,1,0,0,0,0,1,0)
c0 = mean(Y); d0 = var(Y)
MCMCpoissonChange(
  formula = Y~1, m=0, c0=c0, d0=d0,
  burnin = 5000, mcmc = 30000,
  verbose = 0, marginal.likelihood="Chib95") -> model0
MCMCpoissonChange(
  formula = Y~1, m=1, c0=c0, d0=d0, 
  burnin = 5000, mcmc = 30000,
  verbose = 0, marginal.likelihood="Chib95") -> model1

knitr::kable(BayesFactor(model0, model1)$BF.mat)
```



```{r}
plotState(model1)
plotChangepoint(model1)
```


# References
